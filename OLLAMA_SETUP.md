# Настройка Ollama для генерации флеш-карточек

## Что такое Ollama?

Ollama — это инструмент для запуска больших языковых моделей (LLM) локально на вашем компьютере. Это означает, что все данные обрабатываются на вашем устройстве, не отправляясь в облако.

## Шаг 1: Установка Ollama

### macOS

```bash
# Через Homebrew
brew install ollama

# Или скачайте установщик с официального сайта
# https://ollama.ai/download
```

### Windows

1. Скачайте установщик с https://ollama.ai/download
2. Запустите установщик и следуйте инструкциям

### Linux

```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## Шаг 2: Запуск Ollama

После установки Ollama должен автоматически запуститься. Если нет, запустите его вручную:

```bash
ollama serve
```

Ollama будет работать на `http://localhost:11434` (по умолчанию).

## Шаг 3: Скачивание модели

Выберите одну из рекомендованных моделей:

### Вариант 1: Llama 3.2 (3B параметров) - Рекомендуется для начала
```bash
ollama pull llama3.2
```
- ✅ Быстрая и легкая (~2GB)
- ✅ Хорошо работает на обычных компьютерах
- ✅ Достаточно качественная для флеш-карточек

### Вариант 2: Mistral (7B параметров)
```bash
ollama pull mistral
```
- ✅ Лучшее качество
- ⚠️ Требует больше памяти (~4GB)

### Вариант 3: Qwen2.5 (7B параметров)
```bash
ollama pull qwen2.5:7b
```
- ✅ Хорошее качество
- ⚠️ Требует больше памяти (~4GB)

## Шаг 4: Проверка работы

Проверьте, что Ollama работает:

```bash
ollama run llama3.2 "Привет, как дела?"
```

Если видите ответ от модели — всё работает!

## Шаг 5: Настройка в приложении

### Вариант A: Использовать по умолчанию

Измените код в `src/widgets/sidebar/ui/NotesSidebar.tsx`:

```typescript
const cards = await generateFlashcards({
  content: textContent,
  provider: "ollama", // Измените на "ollama"
  model: "llama3.2",  // Укажите скачанную модель
  count: 8,
});
```

### Вариант B: Добавить выбор провайдера в UI

Можно добавить выбор между OpenAI и Ollama в диалоге изучения.

## Шаг 6: Использование

1. Убедитесь, что Ollama запущен (`ollama serve`)
2. Откройте заметку в приложении
3. Нажмите "Изучить" → выберите "Создание флеш-карточек" → "Создать"
4. Флеш-карточки будут сгенерированы локально!

## Настройка URL Ollama (если нужно)

Если Ollama работает на другом порту или хосте, можно указать это в коде:

В `src/shared/api/ai-generator.ts` можно изменить:
```typescript
const ollamaProvider = createAIProvider("ollama", undefined, "http://localhost:11434");
```

## Требования к системе

### Минимальные требования:
- **RAM**: 8GB (для llama3.2), 16GB (для mistral/qwen2.5)
- **Диск**: ~5-10GB свободного места
- **CPU**: Любой современный процессор

### Рекомендуемые требования:
- **RAM**: 16GB+
- **GPU**: Опционально, но значительно ускоряет работу
- **Диск**: SSD для быстрой загрузки модели

## Решение проблем

### Ollama не запускается

```bash
# Проверьте, запущен ли процесс
ps aux | grep ollama

# Если нет, запустите вручную
ollama serve
```

### Модель не найдена

```bash
# Проверьте список установленных моделей
ollama list

# Если нужной модели нет, скачайте её
ollama pull llama3.2
```

### Медленная генерация

1. Используйте более легкую модель (llama3.2 вместо mistral)
2. Убедитесь, что у вас достаточно RAM
3. Если есть GPU, Ollama автоматически его использует

### Ошибка подключения

Убедитесь, что:
- Ollama запущен (`ollama serve`)
- Порт 11434 не занят другим приложением
- Firewall не блокирует подключение

## Преимущества Ollama

✅ **Бесплатно** — никаких лимитов и оплаты  
✅ **Приватно** — данные не покидают ваш компьютер  
✅ **Офлайн** — работает без интернета  
✅ **Без лимитов** — генерируйте сколько угодно карточек  

## Недостатки

⚠️ **Медленнее** — особенно на CPU (может занять 10-30 секунд)  
⚠️ **Требует ресурсов** — нужно достаточно RAM  
⚠️ **Установка** — нужно установить Ollama отдельно  

## Сравнение с OpenAI

| Параметр | Ollama | OpenAI |
|----------|--------|--------|
| Стоимость | Бесплатно | ~$0.15 за 1M токенов |
| Скорость | 10-30 сек | 2-5 сек |
| Приватность | 100% | Данные отправляются в облако |
| Офлайн | Да | Нет |
| Качество | Хорошее | Отличное |

## Рекомендация

Используйте **Ollama**, если:
- Вам важна приватность данных
- Вы хотите работать офлайн
- У вас достаточно RAM (8GB+)
- Вас устраивает немного более медленная генерация

Используйте **OpenAI**, если:
- Нужна максимальная скорость
- Нужно лучшее качество
- Не критична приватность
- Готовы платить за использование

